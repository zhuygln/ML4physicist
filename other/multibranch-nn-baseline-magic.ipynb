{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'sample_submission.csv', 'test.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import gc\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D,AveragePooling2D,MaxPooling2D,Dropout,concatenate\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "#from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions. auc, plot_history\n",
    "def auc(y_true, y_pred):\n",
    "    #auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    y_pred = y_pred.ravel()\n",
    "    y_true = y_true.ravel()\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "def auc_2(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "def plot_history(histories, key='binary_crossentropy'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    #plt.plot([0, 1], [0, 1], 'k--')\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key], '--', label=name.title()+' Val')\n",
    "\n",
    "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(), label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    plt.ylim([0, 0.4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df =  pd.read_csv(\"../input/test.csv\")\n",
    "base_features = [x for x in train_df.columns.values.tolist() if x.startswith('var_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mark real vs fake\n",
    "train_df['real'] = 1\n",
    "\n",
    "for col in base_features:\n",
    "    test_df[col] = test_df[col].map(test_df[col].value_counts())\n",
    "a = test_df[base_features].min(axis=1)\n",
    "\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "test_df['real'] = (a == 1).astype('int')\n",
    "\n",
    "train = train_df.append(test_df).reset_index(drop=True)\n",
    "del test_df, train_df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 20.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# count features\n",
    "for col in tqdm(base_features):\n",
    "    train[col + 'size'] = train[col].map(train.loc[train.real==1, col].value_counts())\n",
    "cnt_features = [col + 'size' for col in base_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [04:06<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# magice features 1\n",
    "for col in tqdm(base_features):\n",
    "#        train[col+'size'] = train.groupby(col)['target'].transform('size')\n",
    "    train.loc[train[col+'size']>1,col+'no_noise'] = train.loc[train[col+'size']>1,col]\n",
    "noise1_features = [col + 'no_noise' for col in base_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA as 0, inspired by lightgbm\n",
    "train[noise1_features] = train[noise1_features].fillna(train[noise1_features].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:44<00:00,  2.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# magice features 2\n",
    "for col in tqdm(base_features):\n",
    "#        train[col+'size'] = train.groupby(col)['target'].transform('size')\n",
    "    train.loc[train[col+'size']>2,col+'no_noise2'] = train.loc[train[col+'size']>2,col]\n",
    "noise2_features = [col + 'no_noise2' for col in base_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA as 0, inspired by lightgbm\n",
    "train[noise2_features] = train[noise2_features].fillna(train[noise2_features].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train[train['target'].notnull()]\n",
    "test_df = train[train['target'].isnull()]\n",
    "all_features = base_features + noise1_features + noise2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "39098e416885d4b96182c53292355a0e49cb0086"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(train_df[all_features].values)\n",
    "df_trn = pd.DataFrame(scaler.transform(train_df[all_features].values), columns=all_features)\n",
    "df_tst = pd.DataFrame(scaler.transform(test_df[all_features].values), columns=all_features)\n",
    "y = train_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(dataset, cols_info):\n",
    "    X = {}\n",
    "    base_feats, noise_feats, noise2_feats = cols_info\n",
    "    X['base'] = np.reshape(np.array(dataset[base_feats].values), (-1, len(base_feats), 1))\n",
    "    X['noise1'] = np.reshape(np.array(dataset[noise_feats].values), (-1, len(noise_feats), 1))\n",
    "    X['noise2'] = np.reshape(np.array(dataset[noise2_feats].values), (-1, len(noise2_feats), 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_info = [base_features, noise1_features, noise2_features]\n",
    "#X = get_keras_data(df_trn[all_features], cols_info)\n",
    "X_test = get_keras_data(df_tst[all_features], cols_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "3afd722cdfbd3a200f5b33dcff2fe33635d02002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "base (InputLayer)               (None, 200, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "noise1 (InputLayer)             (None, 200, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "noise2 (InputLayer)             (None, 200, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200, 16)      32          base[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200, 16)      32          noise1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200, 16)      32          noise2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 200, 16)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 200, 16)      0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 200, 16)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "base_last (Flatten)             (None, 3200)         0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "nose1_last (Flatten)            (None, 3200)         0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "nose2_last (Flatten)            (None, 3200)         0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9600)         0           base_last[0][0]                  \n",
      "                                                                 nose1_last[0][0]                 \n",
      "                                                                 nose2_last[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            9601        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 9,697\n",
      "Trainable params: 9,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define network structure -> 2D CNN\n",
    "def Convnet(cols_info, classes=1):\n",
    "    base_feats, noise1_feats, noise2_feats = cols_info\n",
    "    \n",
    "    # base_feats\n",
    "    X_base_input = Input(shape=(len(base_feats), 1), name='base')\n",
    "    X_base = Dense(16)(X_base_input)\n",
    "    X_base = Activation('relu')(X_base)\n",
    "    X_base = Flatten(name='base_last')(X_base)\n",
    "    \n",
    "    # noise1\n",
    "    X_noise1_input = Input(shape=(len(noise1_feats), 1), name='noise1')\n",
    "    X_noise1 = Dense(16)(X_noise1_input)\n",
    "    X_noise1 = Activation('relu')(X_noise1)\n",
    "    X_noise1 = Flatten(name='nose1_last')(X_noise1)\n",
    "    \n",
    "    # noise2\n",
    "    X_noise2_input = Input(shape=(len(noise2_feats), 1), name='noise2')\n",
    "    X_noise2 = Dense(16)(X_noise2_input)\n",
    "    X_noise2 = Activation('relu')(X_noise2)\n",
    "    X_noise2 = Flatten(name='nose2_last')(X_noise2)\n",
    "    \n",
    "    \n",
    "    X = concatenate([X_base, X_noise1, X_noise2])\n",
    "    X = Dense(classes, activation='sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_base_input, X_noise1_input, X_noise2_input],outputs=X)\n",
    "    \n",
    "    return model\n",
    "model = Convnet(cols_info)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "d2579e2c0abf8be1f0bbe1eec545394475e37568"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    del df_tst\n",
    "except:\n",
    "    pass\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "301805e7d06a14a7ac9087079a2eb1a839626519"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "SEED = 2019\n",
    "n_folds = 5\n",
    "debug_flag = True\n",
    "folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "471b1116f2311ea8f757ee041ec9052aebc9ca57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4c2cc91224d8>:9: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 159999 samples, validate on 40001 samples\n",
      "Epoch 1/300\n",
      "159999/159999 [==============================] - 6s 38us/step - loss: 0.3071 - acc: 0.8966 - binary_crossentropy: 0.3071 - auc_2: 0.7337 - val_loss: 0.2512 - val_acc: 0.9031 - val_binary_crossentropy: 0.2512 - val_auc_2: 0.8543\n",
      "\n",
      "Epoch 00001: val_auc_2 improved from -inf to 0.85431, saving model to NN_fold1.h5\n",
      "Epoch 2/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.2330 - acc: 0.9122 - binary_crossentropy: 0.2330 - auc_2: 0.8673 - val_loss: 0.2272 - val_acc: 0.9161 - val_binary_crossentropy: 0.2272 - val_auc_2: 0.8690\n",
      "\n",
      "Epoch 00002: val_auc_2 improved from 0.85431 to 0.86898, saving model to NN_fold1.h5\n",
      "Epoch 3/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.2211 - acc: 0.9181 - binary_crossentropy: 0.2211 - auc_2: 0.8762 - val_loss: 0.2224 - val_acc: 0.9178 - val_binary_crossentropy: 0.2224 - val_auc_2: 0.8742\n",
      "\n",
      "Epoch 00003: val_auc_2 improved from 0.86898 to 0.87421, saving model to NN_fold1.h5\n",
      "Epoch 4/300\n",
      "159999/159999 [==============================] - 5s 29us/step - loss: 0.2166 - acc: 0.9198 - binary_crossentropy: 0.2166 - auc_2: 0.8815 - val_loss: 0.2186 - val_acc: 0.9192 - val_binary_crossentropy: 0.2186 - val_auc_2: 0.8790\n",
      "\n",
      "Epoch 00004: val_auc_2 improved from 0.87421 to 0.87905, saving model to NN_fold1.h5\n",
      "Epoch 5/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.2127 - acc: 0.9213 - binary_crossentropy: 0.2127 - auc_2: 0.8864 - val_loss: 0.2149 - val_acc: 0.9201 - val_binary_crossentropy: 0.2149 - val_auc_2: 0.8834\n",
      "\n",
      "Epoch 00005: val_auc_2 improved from 0.87905 to 0.88340, saving model to NN_fold1.h5\n",
      "Epoch 6/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.2096 - acc: 0.9225 - binary_crossentropy: 0.2096 - auc_2: 0.8910 - val_loss: 0.2150 - val_acc: 0.9206 - val_binary_crossentropy: 0.2150 - val_auc_2: 0.8882\n",
      "\n",
      "Epoch 00006: val_auc_2 improved from 0.88340 to 0.88825, saving model to NN_fold1.h5\n",
      "Epoch 7/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.2055 - acc: 0.9239 - binary_crossentropy: 0.2055 - auc_2: 0.8948 - val_loss: 0.2080 - val_acc: 0.9227 - val_binary_crossentropy: 0.2080 - val_auc_2: 0.8918\n",
      "\n",
      "Epoch 00007: val_auc_2 improved from 0.88825 to 0.89183, saving model to NN_fold1.h5\n",
      "Epoch 8/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.2014 - acc: 0.9254 - binary_crossentropy: 0.2014 - auc_2: 0.8993 - val_loss: 0.2043 - val_acc: 0.9243 - val_binary_crossentropy: 0.2043 - val_auc_2: 0.8961\n",
      "\n",
      "Epoch 00008: val_auc_2 improved from 0.89183 to 0.89612, saving model to NN_fold1.h5\n",
      "Epoch 9/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1989 - acc: 0.9265 - binary_crossentropy: 0.1989 - auc_2: 0.9027 - val_loss: 0.2011 - val_acc: 0.9251 - val_binary_crossentropy: 0.2011 - val_auc_2: 0.9000\n",
      "\n",
      "Epoch 00009: val_auc_2 improved from 0.89612 to 0.89995, saving model to NN_fold1.h5\n",
      "Epoch 10/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1961 - acc: 0.9275 - binary_crossentropy: 0.1961 - auc_2: 0.9060 - val_loss: 0.1987 - val_acc: 0.9259 - val_binary_crossentropy: 0.1987 - val_auc_2: 0.9026\n",
      "\n",
      "Epoch 00010: val_auc_2 improved from 0.89995 to 0.90262, saving model to NN_fold1.h5\n",
      "Epoch 11/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1932 - acc: 0.9287 - binary_crossentropy: 0.1932 - auc_2: 0.9081 - val_loss: 0.1968 - val_acc: 0.9267 - val_binary_crossentropy: 0.1968 - val_auc_2: 0.9049\n",
      "\n",
      "Epoch 00011: val_auc_2 improved from 0.90262 to 0.90490, saving model to NN_fold1.h5\n",
      "Epoch 12/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1916 - acc: 0.9290 - binary_crossentropy: 0.1916 - auc_2: 0.9103 - val_loss: 0.1952 - val_acc: 0.9278 - val_binary_crossentropy: 0.1952 - val_auc_2: 0.9067\n",
      "\n",
      "Epoch 00012: val_auc_2 improved from 0.90490 to 0.90668, saving model to NN_fold1.h5\n",
      "Epoch 13/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1898 - acc: 0.9295 - binary_crossentropy: 0.1898 - auc_2: 0.9117 - val_loss: 0.1960 - val_acc: 0.9273 - val_binary_crossentropy: 0.1960 - val_auc_2: 0.9082\n",
      "\n",
      "Epoch 00013: val_auc_2 improved from 0.90668 to 0.90817, saving model to NN_fold1.h5\n",
      "Epoch 14/300\n",
      "159999/159999 [==============================] - 5s 29us/step - loss: 0.1889 - acc: 0.9298 - binary_crossentropy: 0.1889 - auc_2: 0.9128 - val_loss: 0.1924 - val_acc: 0.9285 - val_binary_crossentropy: 0.1924 - val_auc_2: 0.9094\n",
      "\n",
      "Epoch 00014: val_auc_2 improved from 0.90817 to 0.90945, saving model to NN_fold1.h5\n",
      "Epoch 15/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1878 - acc: 0.9304 - binary_crossentropy: 0.1878 - auc_2: 0.9142 - val_loss: 0.1954 - val_acc: 0.9278 - val_binary_crossentropy: 0.1954 - val_auc_2: 0.9104\n",
      "\n",
      "Epoch 00015: val_auc_2 improved from 0.90945 to 0.91041, saving model to NN_fold1.h5\n",
      "Epoch 16/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1866 - acc: 0.9308 - binary_crossentropy: 0.1866 - auc_2: 0.9149 - val_loss: 0.1906 - val_acc: 0.9292 - val_binary_crossentropy: 0.1906 - val_auc_2: 0.9112\n",
      "\n",
      "Epoch 00016: val_auc_2 improved from 0.91041 to 0.91116, saving model to NN_fold1.h5\n",
      "Epoch 17/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1856 - acc: 0.9312 - binary_crossentropy: 0.1856 - auc_2: 0.9159 - val_loss: 0.1899 - val_acc: 0.9298 - val_binary_crossentropy: 0.1899 - val_auc_2: 0.9118\n",
      "\n",
      "Epoch 00017: val_auc_2 improved from 0.91116 to 0.91178, saving model to NN_fold1.h5\n",
      "Epoch 18/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1847 - acc: 0.9315 - binary_crossentropy: 0.1847 - auc_2: 0.9167 - val_loss: 0.1895 - val_acc: 0.9299 - val_binary_crossentropy: 0.1895 - val_auc_2: 0.9126\n",
      "\n",
      "Epoch 00018: val_auc_2 improved from 0.91178 to 0.91259, saving model to NN_fold1.h5\n",
      "Epoch 19/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1842 - acc: 0.9318 - binary_crossentropy: 0.1842 - auc_2: 0.9172 - val_loss: 0.1884 - val_acc: 0.9303 - val_binary_crossentropy: 0.1884 - val_auc_2: 0.9134\n",
      "\n",
      "Epoch 00019: val_auc_2 improved from 0.91259 to 0.91338, saving model to NN_fold1.h5\n",
      "Epoch 20/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1834 - acc: 0.9322 - binary_crossentropy: 0.1834 - auc_2: 0.9181 - val_loss: 0.1879 - val_acc: 0.9309 - val_binary_crossentropy: 0.1879 - val_auc_2: 0.9138\n",
      "\n",
      "Epoch 00020: val_auc_2 improved from 0.91338 to 0.91376, saving model to NN_fold1.h5\n",
      "Epoch 21/300\n",
      "159999/159999 [==============================] - 5s 31us/step - loss: 0.1824 - acc: 0.9322 - binary_crossentropy: 0.1824 - auc_2: 0.9186 - val_loss: 0.1876 - val_acc: 0.9306 - val_binary_crossentropy: 0.1876 - val_auc_2: 0.9144\n",
      "\n",
      "Epoch 00021: val_auc_2 improved from 0.91376 to 0.91436, saving model to NN_fold1.h5\n",
      "Epoch 22/300\n",
      "159999/159999 [==============================] - 5s 31us/step - loss: 0.1819 - acc: 0.9325 - binary_crossentropy: 0.1819 - auc_2: 0.9192 - val_loss: 0.1880 - val_acc: 0.9309 - val_binary_crossentropy: 0.1880 - val_auc_2: 0.9148\n",
      "\n",
      "Epoch 00022: val_auc_2 improved from 0.91436 to 0.91480, saving model to NN_fold1.h5\n",
      "Epoch 23/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1812 - acc: 0.9330 - binary_crossentropy: 0.1812 - auc_2: 0.9199 - val_loss: 0.1873 - val_acc: 0.9310 - val_binary_crossentropy: 0.1873 - val_auc_2: 0.9153\n",
      "\n",
      "Epoch 00023: val_auc_2 improved from 0.91480 to 0.91529, saving model to NN_fold1.h5\n",
      "Epoch 24/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1813 - acc: 0.9329 - binary_crossentropy: 0.1813 - auc_2: 0.9202 - val_loss: 0.1868 - val_acc: 0.9310 - val_binary_crossentropy: 0.1868 - val_auc_2: 0.9156\n",
      "\n",
      "Epoch 00024: val_auc_2 improved from 0.91529 to 0.91559, saving model to NN_fold1.h5\n",
      "Epoch 25/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1805 - acc: 0.9333 - binary_crossentropy: 0.1805 - auc_2: 0.9207 - val_loss: 0.1853 - val_acc: 0.9319 - val_binary_crossentropy: 0.1853 - val_auc_2: 0.9161\n",
      "\n",
      "Epoch 00025: val_auc_2 improved from 0.91559 to 0.91606, saving model to NN_fold1.h5\n",
      "Epoch 26/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1796 - acc: 0.9333 - binary_crossentropy: 0.1796 - auc_2: 0.9214 - val_loss: 0.1850 - val_acc: 0.9317 - val_binary_crossentropy: 0.1850 - val_auc_2: 0.9165\n",
      "\n",
      "Epoch 00026: val_auc_2 improved from 0.91606 to 0.91648, saving model to NN_fold1.h5\n",
      "Epoch 27/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1792 - acc: 0.9335 - binary_crossentropy: 0.1792 - auc_2: 0.9217 - val_loss: 0.1869 - val_acc: 0.9310 - val_binary_crossentropy: 0.1869 - val_auc_2: 0.9169\n",
      "\n",
      "Epoch 00027: val_auc_2 improved from 0.91648 to 0.91687, saving model to NN_fold1.h5\n",
      "Epoch 28/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1797 - acc: 0.9332 - binary_crossentropy: 0.1797 - auc_2: 0.9223 - val_loss: 0.1843 - val_acc: 0.9318 - val_binary_crossentropy: 0.1843 - val_auc_2: 0.9171\n",
      "\n",
      "Epoch 00028: val_auc_2 improved from 0.91687 to 0.91706, saving model to NN_fold1.h5\n",
      "Epoch 29/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1784 - acc: 0.9339 - binary_crossentropy: 0.1784 - auc_2: 0.9225 - val_loss: 0.1839 - val_acc: 0.9321 - val_binary_crossentropy: 0.1839 - val_auc_2: 0.9174\n",
      "\n",
      "Epoch 00029: val_auc_2 improved from 0.91706 to 0.91743, saving model to NN_fold1.h5\n",
      "Epoch 30/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1780 - acc: 0.9341 - binary_crossentropy: 0.1780 - auc_2: 0.9230 - val_loss: 0.1858 - val_acc: 0.9314 - val_binary_crossentropy: 0.1858 - val_auc_2: 0.9176\n",
      "\n",
      "Epoch 00030: val_auc_2 improved from 0.91743 to 0.91762, saving model to NN_fold1.h5\n",
      "Epoch 31/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1782 - acc: 0.9338 - binary_crossentropy: 0.1782 - auc_2: 0.9235 - val_loss: 0.1832 - val_acc: 0.9326 - val_binary_crossentropy: 0.1832 - val_auc_2: 0.9180\n",
      "\n",
      "Epoch 00031: val_auc_2 improved from 0.91762 to 0.91799, saving model to NN_fold1.h5\n",
      "Epoch 32/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1772 - acc: 0.9347 - binary_crossentropy: 0.1772 - auc_2: 0.9239 - val_loss: 0.1842 - val_acc: 0.9319 - val_binary_crossentropy: 0.1842 - val_auc_2: 0.9179\n",
      "\n",
      "Epoch 00032: val_auc_2 did not improve from 0.91799\n",
      "Epoch 33/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1765 - acc: 0.9345 - binary_crossentropy: 0.1765 - auc_2: 0.9242 - val_loss: 0.1828 - val_acc: 0.9325 - val_binary_crossentropy: 0.1828 - val_auc_2: 0.9183\n",
      "\n",
      "Epoch 00033: val_auc_2 improved from 0.91799 to 0.91833, saving model to NN_fold1.h5\n",
      "Epoch 34/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1765 - acc: 0.9344 - binary_crossentropy: 0.1765 - auc_2: 0.9246 - val_loss: 0.1833 - val_acc: 0.9323 - val_binary_crossentropy: 0.1833 - val_auc_2: 0.9187\n",
      "\n",
      "Epoch 00034: val_auc_2 improved from 0.91833 to 0.91867, saving model to NN_fold1.h5\n",
      "Epoch 35/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1768 - acc: 0.9344 - binary_crossentropy: 0.1768 - auc_2: 0.9247 - val_loss: 0.1833 - val_acc: 0.9319 - val_binary_crossentropy: 0.1833 - val_auc_2: 0.9188\n",
      "\n",
      "Epoch 00035: val_auc_2 improved from 0.91867 to 0.91881, saving model to NN_fold1.h5\n",
      "Epoch 36/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1757 - acc: 0.9349 - binary_crossentropy: 0.1757 - auc_2: 0.9251 - val_loss: 0.1824 - val_acc: 0.9326 - val_binary_crossentropy: 0.1824 - val_auc_2: 0.9190\n",
      "\n",
      "Epoch 00036: val_auc_2 improved from 0.91881 to 0.91897, saving model to NN_fold1.h5\n",
      "Epoch 37/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1756 - acc: 0.9350 - binary_crossentropy: 0.1756 - auc_2: 0.9255 - val_loss: 0.1820 - val_acc: 0.9330 - val_binary_crossentropy: 0.1820 - val_auc_2: 0.9190\n",
      "\n",
      "Epoch 00037: val_auc_2 did not improve from 0.91897\n",
      "Epoch 38/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1758 - acc: 0.9350 - binary_crossentropy: 0.1758 - auc_2: 0.9257 - val_loss: 0.1818 - val_acc: 0.9334 - val_binary_crossentropy: 0.1818 - val_auc_2: 0.9191\n",
      "\n",
      "Epoch 00038: val_auc_2 improved from 0.91897 to 0.91907, saving model to NN_fold1.h5\n",
      "Epoch 39/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1765 - acc: 0.9345 - binary_crossentropy: 0.1765 - auc_2: 0.9259 - val_loss: 0.1830 - val_acc: 0.9322 - val_binary_crossentropy: 0.1830 - val_auc_2: 0.9193\n",
      "\n",
      "Epoch 00039: val_auc_2 improved from 0.91907 to 0.91935, saving model to NN_fold1.h5\n",
      "Epoch 40/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1749 - acc: 0.9351 - binary_crossentropy: 0.1749 - auc_2: 0.9263 - val_loss: 0.1815 - val_acc: 0.9334 - val_binary_crossentropy: 0.1815 - val_auc_2: 0.9193\n",
      "\n",
      "Epoch 00040: val_auc_2 did not improve from 0.91935\n",
      "Epoch 41/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1744 - acc: 0.9354 - binary_crossentropy: 0.1744 - auc_2: 0.9263 - val_loss: 0.1815 - val_acc: 0.9333 - val_binary_crossentropy: 0.1815 - val_auc_2: 0.9194\n",
      "\n",
      "Epoch 00041: val_auc_2 improved from 0.91935 to 0.91938, saving model to NN_fold1.h5\n",
      "Epoch 42/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1741 - acc: 0.9354 - binary_crossentropy: 0.1741 - auc_2: 0.9266 - val_loss: 0.1813 - val_acc: 0.9335 - val_binary_crossentropy: 0.1813 - val_auc_2: 0.9195\n",
      "\n",
      "Epoch 00042: val_auc_2 improved from 0.91938 to 0.91954, saving model to NN_fold1.h5\n",
      "Epoch 43/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1738 - acc: 0.9357 - binary_crossentropy: 0.1738 - auc_2: 0.9269 - val_loss: 0.1814 - val_acc: 0.9330 - val_binary_crossentropy: 0.1814 - val_auc_2: 0.9196\n",
      "\n",
      "Epoch 00043: val_auc_2 improved from 0.91954 to 0.91964, saving model to NN_fold1.h5\n",
      "Epoch 44/300\n",
      "159999/159999 [==============================] - 5s 31us/step - loss: 0.1739 - acc: 0.9357 - binary_crossentropy: 0.1739 - auc_2: 0.9272 - val_loss: 0.1833 - val_acc: 0.9315 - val_binary_crossentropy: 0.1833 - val_auc_2: 0.9198\n",
      "\n",
      "Epoch 00044: val_auc_2 improved from 0.91964 to 0.91979, saving model to NN_fold1.h5\n",
      "Epoch 45/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1736 - acc: 0.9356 - binary_crossentropy: 0.1736 - auc_2: 0.9272 - val_loss: 0.1824 - val_acc: 0.9328 - val_binary_crossentropy: 0.1824 - val_auc_2: 0.9199\n",
      "\n",
      "Epoch 00045: val_auc_2 improved from 0.91979 to 0.91987, saving model to NN_fold1.h5\n",
      "Epoch 46/300\n",
      "159999/159999 [==============================] - 5s 29us/step - loss: 0.1736 - acc: 0.9357 - binary_crossentropy: 0.1736 - auc_2: 0.9274 - val_loss: 0.1809 - val_acc: 0.9338 - val_binary_crossentropy: 0.1809 - val_auc_2: 0.9200\n",
      "\n",
      "Epoch 00046: val_auc_2 improved from 0.91987 to 0.91996, saving model to NN_fold1.h5\n",
      "Epoch 47/300\n",
      "159999/159999 [==============================] - 5s 29us/step - loss: 0.1733 - acc: 0.9359 - binary_crossentropy: 0.1733 - auc_2: 0.9274 - val_loss: 0.1814 - val_acc: 0.9330 - val_binary_crossentropy: 0.1814 - val_auc_2: 0.9200\n",
      "\n",
      "Epoch 00047: val_auc_2 improved from 0.91996 to 0.92004, saving model to NN_fold1.h5\n",
      "Epoch 48/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1729 - acc: 0.9360 - binary_crossentropy: 0.1729 - auc_2: 0.9279 - val_loss: 0.1810 - val_acc: 0.9336 - val_binary_crossentropy: 0.1810 - val_auc_2: 0.9200\n",
      "\n",
      "Epoch 00048: val_auc_2 did not improve from 0.92004\n",
      "Epoch 49/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1728 - acc: 0.9360 - binary_crossentropy: 0.1728 - auc_2: 0.9278 - val_loss: 0.1814 - val_acc: 0.9322 - val_binary_crossentropy: 0.1814 - val_auc_2: 0.9201\n",
      "\n",
      "Epoch 00049: val_auc_2 improved from 0.92004 to 0.92011, saving model to NN_fold1.h5\n",
      "Epoch 50/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1728 - acc: 0.9358 - binary_crossentropy: 0.1728 - auc_2: 0.9280 - val_loss: 0.1808 - val_acc: 0.9330 - val_binary_crossentropy: 0.1808 - val_auc_2: 0.9202\n",
      "\n",
      "Epoch 00050: val_auc_2 improved from 0.92011 to 0.92016, saving model to NN_fold1.h5\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 51/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1716 - acc: 0.9365 - binary_crossentropy: 0.1716 - auc_2: 0.9287 - val_loss: 0.1812 - val_acc: 0.9326 - val_binary_crossentropy: 0.1812 - val_auc_2: 0.9203\n",
      "\n",
      "Epoch 00051: val_auc_2 improved from 0.92016 to 0.92027, saving model to NN_fold1.h5\n",
      "Epoch 52/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1722 - acc: 0.9361 - binary_crossentropy: 0.1722 - auc_2: 0.9288 - val_loss: 0.1830 - val_acc: 0.9329 - val_binary_crossentropy: 0.1830 - val_auc_2: 0.9202\n",
      "\n",
      "Epoch 00052: val_auc_2 did not improve from 0.92027\n",
      "Epoch 53/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1716 - acc: 0.9365 - binary_crossentropy: 0.1716 - auc_2: 0.9291 - val_loss: 0.1806 - val_acc: 0.9337 - val_binary_crossentropy: 0.1806 - val_auc_2: 0.9202\n",
      "\n",
      "Epoch 00053: val_auc_2 did not improve from 0.92027\n",
      "Epoch 54/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1717 - acc: 0.9363 - binary_crossentropy: 0.1717 - auc_2: 0.9289 - val_loss: 0.1806 - val_acc: 0.9339 - val_binary_crossentropy: 0.1806 - val_auc_2: 0.9202\n",
      "\n",
      "Epoch 00054: val_auc_2 did not improve from 0.92027\n",
      "Epoch 55/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1714 - acc: 0.9366 - binary_crossentropy: 0.1714 - auc_2: 0.9288 - val_loss: 0.1805 - val_acc: 0.9337 - val_binary_crossentropy: 0.1805 - val_auc_2: 0.9202\n",
      "\n",
      "Epoch 00055: val_auc_2 did not improve from 0.92027\n",
      "Epoch 56/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1712 - acc: 0.9367 - binary_crossentropy: 0.1712 - auc_2: 0.9292 - val_loss: 0.1806 - val_acc: 0.9339 - val_binary_crossentropy: 0.1806 - val_auc_2: 0.9203\n",
      "\n",
      "Epoch 00056: val_auc_2 improved from 0.92027 to 0.92028, saving model to NN_fold1.h5\n",
      "Epoch 57/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1715 - acc: 0.9365 - binary_crossentropy: 0.1715 - auc_2: 0.9292 - val_loss: 0.1805 - val_acc: 0.9335 - val_binary_crossentropy: 0.1805 - val_auc_2: 0.9203\n",
      "\n",
      "Epoch 00057: val_auc_2 improved from 0.92028 to 0.92034, saving model to NN_fold1.h5\n",
      "Epoch 58/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1714 - acc: 0.9366 - binary_crossentropy: 0.1714 - auc_2: 0.9292 - val_loss: 0.1806 - val_acc: 0.9331 - val_binary_crossentropy: 0.1806 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00058: val_auc_2 improved from 0.92034 to 0.92037, saving model to NN_fold1.h5\n",
      "Epoch 59/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1713 - acc: 0.9366 - binary_crossentropy: 0.1713 - auc_2: 0.9293 - val_loss: 0.1807 - val_acc: 0.9331 - val_binary_crossentropy: 0.1807 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00059: val_auc_2 improved from 0.92037 to 0.92041, saving model to NN_fold1.h5\n",
      "Epoch 60/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1712 - acc: 0.9366 - binary_crossentropy: 0.1712 - auc_2: 0.9294 - val_loss: 0.1804 - val_acc: 0.9334 - val_binary_crossentropy: 0.1804 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00060: val_auc_2 improved from 0.92041 to 0.92045, saving model to NN_fold1.h5\n",
      "Epoch 61/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1711 - acc: 0.9366 - binary_crossentropy: 0.1711 - auc_2: 0.9293 - val_loss: 0.1803 - val_acc: 0.9335 - val_binary_crossentropy: 0.1803 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00061: val_auc_2 did not improve from 0.92045\n",
      "Epoch 62/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1709 - acc: 0.9367 - binary_crossentropy: 0.1709 - auc_2: 0.9297 - val_loss: 0.1804 - val_acc: 0.9340 - val_binary_crossentropy: 0.1804 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00062: val_auc_2 did not improve from 0.92045\n",
      "Epoch 63/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1709 - acc: 0.9368 - binary_crossentropy: 0.1709 - auc_2: 0.9293 - val_loss: 0.1803 - val_acc: 0.9338 - val_binary_crossentropy: 0.1803 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00063: val_auc_2 did not improve from 0.92045\n",
      "Epoch 64/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1710 - acc: 0.9367 - binary_crossentropy: 0.1710 - auc_2: 0.9295 - val_loss: 0.1803 - val_acc: 0.9336 - val_binary_crossentropy: 0.1803 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00064: val_auc_2 did not improve from 0.92045\n",
      "Epoch 65/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1708 - acc: 0.9369 - binary_crossentropy: 0.1708 - auc_2: 0.9296 - val_loss: 0.1803 - val_acc: 0.9338 - val_binary_crossentropy: 0.1803 - val_auc_2: 0.9205\n",
      "\n",
      "Epoch 00065: val_auc_2 improved from 0.92045 to 0.92045, saving model to NN_fold1.h5\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 66/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1702 - acc: 0.9371 - binary_crossentropy: 0.1702 - auc_2: 0.9301 - val_loss: 0.1802 - val_acc: 0.9337 - val_binary_crossentropy: 0.1802 - val_auc_2: 0.9205\n",
      "\n",
      "Epoch 00066: val_auc_2 improved from 0.92045 to 0.92049, saving model to NN_fold1.h5\n",
      "Epoch 67/300\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.1702 - acc: 0.9369 - binary_crossentropy: 0.1702 - auc_2: 0.9301 - val_loss: 0.1803 - val_acc: 0.9338 - val_binary_crossentropy: 0.1803 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00067: val_auc_2 did not improve from 0.92049\n",
      "Epoch 68/300\n",
      "159999/159999 [==============================] - 5s 29us/step - loss: 0.1702 - acc: 0.9371 - binary_crossentropy: 0.1702 - auc_2: 0.9300 - val_loss: 0.1803 - val_acc: 0.9339 - val_binary_crossentropy: 0.1803 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00068: val_auc_2 did not improve from 0.92049\n",
      "Epoch 69/300\n",
      "159999/159999 [==============================] - 5s 29us/step - loss: 0.1701 - acc: 0.9370 - binary_crossentropy: 0.1701 - auc_2: 0.9301 - val_loss: 0.1814 - val_acc: 0.9336 - val_binary_crossentropy: 0.1814 - val_auc_2: 0.9204\n",
      "\n",
      "Epoch 00069: val_auc_2 did not improve from 0.92049\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 70/300\n",
      " 30720/159999 [====>.........................] - ETA: 3s - loss: 0.1734 - acc: 0.9356 - binary_crossentropy: 0.1734 - auc_2: 0.9286"
     ]
    }
   ],
   "source": [
    "#transformed_shape = tuple([-1] + list(shape))\n",
    "#X_test = np.reshape(X_test, transformed_shape)\n",
    "\n",
    "i = 0\n",
    "result = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "val_aucs = []\n",
    "valid_X = train_df[['target']]\n",
    "valid_X['predict'] = 0\n",
    "for train_idx, val_idx in skf.split(df_trn, y):\n",
    "    if i == folds:\n",
    "        break\n",
    "    i += 1    \n",
    "    X_train, y_train = df_trn.iloc[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = df_trn.iloc[val_idx], y[val_idx]\n",
    "    \n",
    "    X_train = get_keras_data(X_train, cols_info)\n",
    "    X_valid = get_keras_data(X_valid, cols_info)\n",
    "    #X_train = np.reshape(X_train, transformed_shape)\n",
    "    #X_valid = np.reshape(X_valid, transformed_shape)\n",
    "    \n",
    "    model_name = 'NN_fold{}.h5'.format(str(i))\n",
    "    \n",
    "    model = Convnet(cols_info)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy', auc_2])\n",
    "    checkpoint = ModelCheckpoint(model_name, monitor='val_auc_2', verbose=1, \n",
    "                                 save_best_only=True, mode='max', save_weights_only = True)\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n",
    "                                       verbose=1, mode='min', epsilon=0.0001)\n",
    "    earlystop = EarlyStopping(monitor='val_auc_2', mode='max', patience=10, verbose=1)\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=300, \n",
    "                        batch_size=1024 * 2, \n",
    "                        validation_data=(X_valid, y_valid), \n",
    "                        callbacks=[checkpoint, reduceLROnPlat, earlystop])\n",
    "    train_history = pd.DataFrame(history.history)\n",
    "    train_history.to_csv('train_profile_fold{}.csv'.format(str(i)), index=None)\n",
    "    \n",
    "    # load and predict\n",
    "    model.load_weights(model_name)\n",
    "    \n",
    "    #predict\n",
    "    y_pred_keras = model.predict(X_valid).ravel()\n",
    "    \n",
    "    # AUC\n",
    "    valid_X['predict'].iloc[val_idx] = y_pred_keras\n",
    "    \n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_valid, y_pred_keras)\n",
    "    auc_valid = roc_auc_score(y_valid, y_pred_keras)\n",
    "    val_aucs.append(auc_valid)\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    result[\"fold{}\".format(str(i))] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "cf12c8076b868e0f1228fd2884b14f86a87c0c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold_1 AUC: 0.920138\n",
      "Fold_2 AUC: 0.918263\n",
      "Fold_3 AUC: 0.923050\n",
      "Fold_4 AUC: 0.919308\n",
      "Fold_5 AUC: 0.917681\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_aucs)):\n",
    "    print('Fold_%d AUC: %.6f' % (i+1, val_aucs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "f5b28c3ad1a96e4edd711546667cbac1527d57c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold auc mean: 0.919687893, std: 0.001881866. All auc: 0.919616.\n"
     ]
    }
   ],
   "source": [
    "# summary on results\n",
    "auc_mean = np.mean(val_aucs)\n",
    "auc_std = np.std(val_aucs)\n",
    "auc_all = roc_auc_score(valid_X.target, valid_X.predict)\n",
    "print('%d-fold auc mean: %.9f, std: %.9f. All auc: %6f.' % (n_folds, auc_mean, auc_std, auc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "e978483f1836a293a76bcf54d27cec81905a3667"
   },
   "outputs": [],
   "source": [
    "y_all = result.values[:, 1:]\n",
    "result['target'] = np.mean(y_all, axis = 1)\n",
    "to_submit = result[['ID_code', 'target']]\n",
    "to_submit.to_csv('NN_submission.csv', index=None)\n",
    "result.to_csv('NN_all_prediction.csv', index=None)\n",
    "valid_X['ID_code'] = train_df['ID_code']\n",
    "valid_X = valid_X[['ID_code', 'target', 'predict']].to_csv('NN_oof.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "bc08d7dc3cce9901126e935471f94203e48804ea"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
